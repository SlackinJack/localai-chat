##################################################
############ BEGIN MAIN CONFIGURATION ############
##################################################


# address of the server
ADDRESS=http://localhost:8080/v1


# set the completion model that will handle
# automatic model switching (if enabled)
# (it is recommended to set a model NOT included in models.json)
# (most models tend to self-bias, go figure.)
# otherwise, set the model you want to use
DEFAULT_MODEL=catppt


# toggle automatic model switching
# can be useful if you use a lot of different models
# and dont want to manually switch them every time
# beta!
ENABLE_AUTOMATIC_MODEL_SWITCHING=True


# enable considering conversation history (it will still write to file!)
# should disable if you are doing lots of unrelated prompts (eg. code generation)
# if you disable, i would suggest using 'burner conversation files'
# toggle at runtime with 'historyon' and 'historyoff'
CHAT_HISTORY_CONSIDERATION=False


# enable searching online
# useful if you want to strictly generate text (eg. code generation)
# you can toggle this at runtime with 'online' and 'offline'
ENABLE_INTERNET=False


# enable loopback on search
# it can be useful for complex prompts that require
# multiple searches for differing topics (eg. compare this vs that, pros and cons, etc)
# this can astronomically increase prompt times, especially on low-end hardware
# this will also add to future prompt times on the same conversation
# disable to prompt immediately after searching once
SEARCH_LOOPBACK=True


# max times to loopback
# set to a realistic number
# but not too low that we cant a good dataset
MAX_SEARCH_LOOPBACK_ITERATIONS=5


# set debug level
# 4 - all
# 3 - debug
# 2 - info
# 1 - error
# 0 - disable
DEBUG_LEVEL=4


# max number of sentences to consider
# when using online sources
# low values may affect data quality
# high values will increase processing time
# remember to (in/de)crease context size
MAX_SENTENCES=12


# max number of sources to consider
# when using online sources
# low values may affect data diversity
# high values will increase processing time
# remember to (in/de)crease context size 
MAX_SOURCES=2


# automatically open files on completion
# eg. generated images, audio files, etc.
AUTO_OPEN_FILES=True


# UwU-ify your assistant.................
# yeah, no comment.
# outputs may literally be illegible
# very beta, probably broken to some extent
UWU_IFY=False


#################################################
###### BEGIN STABLEDIFFUSION CONFIGURATION ######
#################################################


# set default sd model
STABLE_DIFFUSION_MODEL=stablediffusion


# set image size
IMAGE_SIZE=256x256


#################################################
######### BEGIN TEMPLATES CONFIGURATION #########
#################################################


# this is the description for function_result
# the goal is to get the assistant to determine the next action
TEMPLATE_FUNCTION_RESULT_DESCRIPTION=Reply with the next appropriate action.


# this is the description for search_terms in function_result
# the goal is to describe how a search term should look,
# while having enough diversity and coverage to look for good sources for the prompt
TEMPLATE_FUNCTION_RESULT_SEARCH_TERMS_DESCRIPTION=A small set of keywords, or a short search term, for the question that you are answering. It must be minimal, specific, and adaquately descriptive.


# this is the system prompt when sending the promptHistory
# along with the user prompt to determine the next function_result
TEMPLATE_FUNCTION_RESULT_SYSTEM_PROMPT=You are a helpful assistant. Your goal is to reply factually to the conversation. Determine the next appropriate action.


# this is the system prompt for a generic chat completion request
TEMPLATE_CHAT_COMPLETION_SYSTEM_PROMPT=You are a helpful assistant. You will respond to USER. Keep your response brief and relevant to the task.


# this is the uwu-system prompt for a generic chat completion request
# your mileage will vary depending on your chat model:
TEMPLATE_CHAT_COMPLETION_SYSTEM_PROMPT_UWU=You are a helpful assistant. You will reply to the user. In your response, you will: Stutter your responses; Add umms and uhhs as verbal pauses in sentences; Reply with uncertainty and doubt; Show immaturity; Interject short and impulsive thoughts into your responses.


# this is the system prompt for model response
TEMPLATE_MODEL_SYSTEM_PROMPT=Which assistant has the most relevant skills related to the task given by USER?

