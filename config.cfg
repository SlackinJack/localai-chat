##################################################
############ BEGIN MAIN CONFIGURATION ############
##################################################


# address of the server
ADDRESS=http://localhost:8080/v1


# set default chat model
# leave empty for auto
# this is the 'front-end' model
# CHAT_MODEL=jordan-7b


# set default prompt model
# leave empty for auto
# this is the 'back-end' model
DEFAULT_MODEL=jordan-7b


# enable considering conversation history (it will still write to file!)
# should disable if you are doing lots of unrelated prompts (eg. code generation)
# if you disable, i would suggest using 'burner conversation files'
# toggle at runtime with 'historyon' and 'historyoff'
CHAT_HISTORY_CONSIDERATION=False


# enable searching online
# useful if you want to strictly generate text (eg. code generation)
# you can toggle this at runtime with 'online' and 'offline'
ENABLE_INTERNET=False


# enable loopback on search
# it can be useful for complex prompts that require
# multiple searches for differing topics (eg. compare this vs that, pros and cons, etc)
# this can astronomically increase prompt times, especially on low-end hardware
# this will also add to future prompt times on the same conversation
# disable to prompt immediately after searching once
SEARCH_LOOPBACK=True


# max times to loopback
# set to a realistic number
# but not too low that we cant a good dataset
MAX_SEARCH_LOOPBACK_ITERATIONS=5


# set debug level
# 4 - all
# 3 - debug
# 2 - info
# 1 - error
# 0 - disable
DEBUG_LEVEL=4


# models to ignore (stablediffusion models, folders, etc.)
IGNORED_MODELS=stablediffusion,assets,configs,lunademo,example,test


# max number of sentences to consider
# when using online sources
# low values may affect data quality
# high values will increase processing time
# remember to (in/de)crease context size
MAX_SENTENCES=12


# max number of sources to consider
# when using online sources
# low values may affect data diversity
# high values will increase processing time
# remember to (in/de)crease context size 
MAX_SOURCES=2


# automatically open files on completion
# eg. generated images, audio files, etc.
AUTO_OPEN_FILES=True


# UwU-ify your assistant.................
# yeah, no comment.
# outputs may literally be illegible
# very beta, probably broken to some extent
UWU_IFY=False


#################################################
###### BEGIN STABLEDIFFUSION CONFIGURATION ######
#################################################


# set default sd model
STABLE_DIFFUSION_MODEL=stablediffusion


# set image size
IMAGE_SIZE=256x256


#################################################
######### BEGIN TEMPLATES CONFIGURATION #########
#################################################


# this is the description for function_result
# the goal is to get the assistant to determine the next action
TEMPLATE_FUNCTION_RESULT_DESCRIPTION=Reply with the next appropriate action.


# this is the description for search_terms in function_result
# the goal is to describe how a search term should look,
# while having enough diversity and coverage to look for good sources for the prompt
TEMPLATE_FUNCTION_RESULT_SEARCH_TERMS_DESCRIPTION=A small set of keywords, or a short search term, for the question that you are answering. It must be minimal, specific, and adaquately descriptive.


# this is the system prompt when sending the promptHistory
# along with the user prompt to determine the next function_result
TEMPLATE_FUNCTION_RESULT_SYSTEM_PROMPT=You are a helpful assistant. Your goal is to reply factually to the conversation. Determine the next appropriate action.


# this is the system prompt for a generic chat completion request
TEMPLATE_CHAT_COMPLETION_SYSTEM_PROMPT=You are a helpful assistant. You will reply to the user.


# this is the uwu-system prompt for a generic chat completion request
# your mileage will vary depending on your chat model:
TEMPLATE_CHAT_COMPLETION_SYSTEM_PROMPT_UWU=You are a helpful assistant. You will reply to the user. In your response, you will: Stutter your responses; Add umms and uhhs as verbal pauses in sentences; Reply with uncertainty and doubt; Show immaturity; Interject short and impulsive thoughts into your responses.


# this is the system prompt for determining which model to use for the next completion
TEMPLATE_MODEL_COMPLETION_SYSTEM_PROMPT=Determine the best assistant to respond to the following inquiry: 

